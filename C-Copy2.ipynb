{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZH2fECAHndSt"
   },
   "source": [
    "C MODEL\n",
    "========\n",
    "\n",
    "This model is a bit more sophisticated. It is a recursive model which will generate results using the embeddings of the text as well as the penultimate layer of the previous processed writtings. \n",
    "\n",
    "The different writtings are put trhough an TF-IDF tokenizer, this will be the input to the next stages.\n",
    "\n",
    "The tokenized data is then put trough a Neural Network model, which will learn from it to later classify the server occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IvqZXJ-V_Js-"
   },
   "source": [
    "Initialization of the Environment\n",
    "=========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9E9I1vRD_GXq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OZj-8fP8AOCV"
   },
   "source": [
    "Load Data\n",
    "========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8nIMHcEC0iP"
   },
   "outputs": [],
   "source": [
    "SST_HOME='/datos/erisk/eRisk/'\n",
    "path_train=SST_HOME+'dataset/t2_training_data/train.csv'\n",
    "path_test=SST_HOME+'dataset/t2_test_data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xCTjbtoaCadB"
   },
   "outputs": [],
   "source": [
    "def xml_csv(path): \n",
    "  xml_list = []\n",
    "  xml_list_test = []\n",
    "  rate = 0.8 \n",
    "  i = 0\n",
    "  files = glob.glob(path+'/*.xml')\n",
    "  random.shuffle(files)\n",
    "  train = int(len(files)*rate)\n",
    "  for xml_file in files: \n",
    "    i = i+1 \n",
    "    #print(xml_file)\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    if i <= train: \n",
    "      subject_id = root.find('ID').text\n",
    "      for writing in root.findall('WRITING'): \n",
    "        writing_data = {}\n",
    "        writing_data['ID'] = subject_id\n",
    "        writing_data['TITLE'] = writing.find('TITLE').text\n",
    "        writing_data['DATE'] = writing.find('DATE').text\n",
    "        writing_data['TEXT'] = writing.find('TEXT').text\n",
    "        writing_data['INFO'] = writing.find('INFO').text\n",
    "        xml_list.append(writing_data)\n",
    "\n",
    "    else: \n",
    "      subject_id = root.find('ID').text\n",
    "      for writing in root.findall('WRITING'): \n",
    "        writing_data = {}\n",
    "        writing_data['ID'] = subject_id\n",
    "        writing_data['TITLE'] = writing.find('TITLE').text\n",
    "        writing_data['DATE'] = writing.find('DATE').text\n",
    "        writing_data['TEXT'] = writing.find('TEXT').text\n",
    "        writing_data['INFO'] = writing.find('INFO').text\n",
    "        xml_list_test.append(writing_data)\n",
    "  print(i)\n",
    "  column_name= ['ID','DATE','TITLE','TEXT','INFO']\n",
    "  xml_df = pd.DataFrame(xml_list,columns=column_name)\n",
    "  xml_df = xml_df.set_index(['ID', 'DATE'])\n",
    "  xml_df = xml_df.sort_index()\n",
    "  xml_df_test = pd.DataFrame(xml_list_test,columns=column_name)\n",
    "  xml_df_test = xml_df_test.set_index(['ID', 'DATE'])\n",
    "  xml_df_test = xml_df_test.sort_index()\n",
    "  with open(path_risk) as gt:\n",
    "    print('llego')\n",
    "    xml_df[\"LABEL\"] = np.nan\n",
    "    xml_df_test[\"LABEL\"] = np.nan\n",
    "    for line in gt:\n",
    "      if line == '\\n':\n",
    "        break\n",
    "      rec_id, value = line.split(' ', 1 );\n",
    "      #print(f\"El usuario {rec_id} es {value}\")\n",
    "      if rec_id in xml_df.index: \n",
    "        xml_df.loc[rec_id, 'LABEL'] = int(value)\n",
    "        #print(xml_df.loc[rec_id])\n",
    "      else: \n",
    "        xml_df_test.loc[rec_id, 'LABEL'] = int(value)\n",
    "  xml_df['LABEL'] = xml_df['LABEL'].astype('int')\n",
    "  xml_df_test['LABEL'] = xml_df_test['LABEL'].astype('int')\n",
    "  xml_df.to_csv(path_train)\n",
    "  xml_df_test.to_csv(path_test)\n",
    "\n",
    "  return xml_df, xml_df_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5096,
     "status": "ok",
     "timestamp": 1562651910224,
     "user": {
      "displayName": "PABLO RAEZ GARCIA-RETAMERO",
      "photoUrl": "",
      "userId": "03506494368259470344"
     },
     "user_tz": -120
    },
    "id": "3BsYEF8EPKcn",
    "outputId": "0b78444e-80d4-45a0-ae9d-e13d4fbc2ebe"
   },
   "outputs": [],
   "source": [
    "path = '/datos/erisk/eRisk/dataset/t2_training_data/eRisk2021_T1/data'#path donde guarda los ficheros xml \n",
    "path_risk = '/datos/erisk/eRisk/dataset/t2_training_data/eRisk2021_T1/risk_golden_truth.txt'\n",
    "train_df,test_df = xml_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ty-pFhXRffvO"
   },
   "outputs": [],
   "source": [
    "train_x_df = train_df[[\"TEXT\", \"TITLE\"]]\n",
    "train_y_df = train_df[\"LABEL\"]\n",
    "\n",
    "test_x_df = test_df[[\"TEXT\", \"TITLE\"]]\n",
    "test_y_df = test_df[\"LABEL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OR_vPgp_bsLp"
   },
   "source": [
    "A Model Text\n",
    "==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3ylNsFrBYUb"
   },
   "outputs": [],
   "source": [
    "train_x = [str(text) + str(title) for text, title in train_df[[\"TEXT\", \"TITLE\"]].values]\n",
    "train_y = train_df[\"LABEL\"].values\n",
    "\n",
    "test_x = [str(text) + str(title) for text, title in test_df[[\"TEXT\", \"TITLE\"]].values]\n",
    "test_y = test_df[\"LABEL\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5487,
     "status": "ok",
     "timestamp": 1562651912402,
     "user": {
      "displayName": "PABLO RAEZ GARCIA-RETAMERO",
      "photoUrl": "",
      "userId": "03506494368259470344"
     },
     "user_tz": -120
    },
    "id": "wlTUskKxqlO-",
    "outputId": "1da8deeb-0225-4b5c-fd0e-722fb5f1cbf3"
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words=stopwords.words('english')\n",
    "# Define maximum vocabulary length\n",
    "MAX_WORDS = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVPBM3kJwtaK"
   },
   "outputs": [],
   "source": [
    "# We are using this function to clean the test set\n",
    "def tokenize_clean_text(text, tfidf=True, tokenizer=None, max_length=None, max_words=MAX_WORDS):\n",
    "  \"\"\"\n",
    "  This function is in charge of tokenizing the text it is given. It also cleans\n",
    "  the text from stop-words, punctuation, and gives a special token to numbers.\n",
    "  \n",
    "  :param text: The texts to tokenize in a bidimensional python array.\n",
    "  \n",
    "  :returns: The tokenized and cleaned text in a bidimensional python array.\n",
    "            The tokenizer used to preprocess the text.\n",
    "            The maximum length used for padding.\n",
    "  \"\"\"   \n",
    "  # set [removed] as a special token\n",
    "  text_removed = [t.replace(\"[removed]\", \"R3MOV3D\") for t in text]\n",
    "  \n",
    "  # We remove the numbers\n",
    "  cropped_numbers_text = [\" \".join([word if not word.isdigit() else \"\"\n",
    "                                for word in sentence.split()])\n",
    "                               for sentence in text_removed]\n",
    "  \n",
    "  # Delete stopwords as well as every word less than 3 chars.\n",
    "  cropped_numbers_stopw_text = [\" \".join([word if not (word in stop_words or len(word) <= 3) else \"\"\n",
    "                                      for word in sentence.split()])\n",
    "                                     for sentence in cropped_numbers_text]\n",
    "  \n",
    "  if tfidf:\n",
    "    vec = TfidfVectorizer(max_features=max_words)\n",
    "    tfidf_mat = vec.fit_transform(cropped_numbers_stopw_text).toarray()\n",
    "    tfid_words = vec.get_feature_names_out()\n",
    "\n",
    "    cropped_numbers_stopw_tfidf_text = [\" \".join([word if word in tfid_words else \"\"\n",
    "                                            for word in sentence.split()])\n",
    "                                            for sentence in cropped_numbers_stopw_text]\n",
    "  \n",
    "  if tokenizer is None:\n",
    "    tokenizer = Tokenizer(num_words=max_words) # They use 5k words too\n",
    "    tokenizer.fit_on_texts(cropped_numbers_stopw_tfidf_text if tfidf else cropped_numbers_stopw_text)\n",
    "  # We tokenize the sentences\n",
    "  tokenized_text = tokenizer.texts_to_sequences(cropped_numbers_stopw_tfidf_text if tfidf else cropped_numbers_stopw_text)\n",
    "  \n",
    "  if max_length == None:\n",
    "    max_length = 0\n",
    "    for sentence in tokenized_text:\n",
    "      max_length = max_length if len(sentence) < max_length else len(sentence)\n",
    "  \n",
    "  # Now we return the padded the sequences.\n",
    "  return pad_sequences(tokenized_text, max_length), tokenizer, max_length, cropped_numbers_stopw_tfidf_text if tfidf else cropped_numbers_stopw_text\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UARLdwsDya70",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x_token, tokenizer, max_length, train_x_clean = tokenize_clean_text(train_x) \n",
    "test_x_token, _, _, test_x_clean = tokenize_clean_text(test_x, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 334984,
     "status": "ok",
     "timestamp": 1562652242357,
     "user": {
      "displayName": "PABLO RAEZ GARCIA-RETAMERO",
      "photoUrl": "",
      "userId": "03506494368259470344"
     },
     "user_tz": -120
    },
    "id": "94n2g_Q5mcga",
    "outputId": "530a2339-7504-4a97-f120-56254fc9ee81"
   },
   "outputs": [],
   "source": [
    "# Save the tokenizer for test stage pruposes\n",
    "import joblib\n",
    "\n",
    "PATH_TOKENIZER = SST_HOME + \"DL/tokenizer.pkl\"\n",
    "\n",
    "joblib.dump(tokenizer, PATH_TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f6hPe4yddMl3"
   },
   "outputs": [],
   "source": [
    "token_index = np.unique(train_x_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GZnZKal_kNst"
   },
   "outputs": [],
   "source": [
    "max_words = len(token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B2Q6xmWNUsri"
   },
   "outputs": [],
   "source": [
    "# the length will be set to 100\n",
    "max_length = 50\n",
    "train_x_token_cropp = train_x_token[:,-max_length:]\n",
    "test_x_token_cropp = test_x_token[:,-max_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 338617,
     "status": "ok",
     "timestamp": 1562652247282,
     "user": {
      "displayName": "PABLO RAEZ GARCIA-RETAMERO",
      "photoUrl": "",
      "userId": "03506494368259470344"
     },
     "user_tz": -120
    },
    "id": "VfAjD6SUh6Dz",
    "outputId": "5a96d8f5-6017-4f36-c2cf-141b559b01e1"
   },
   "outputs": [],
   "source": [
    "assert len(train_x_token) == len(train_x_df)\n",
    "assert len(train_y) == len(train_y_df)\n",
    "assert len(test_x_token) == len(test_x_df)\n",
    "assert len(test_y) == len(test_y_df)\n",
    "\n",
    "train_x_df[\"TOKENIZED\"] = train_x_token_cropp.tolist()\n",
    "test_x_df[\"TOKENIZED\"] = test_x_token_cropp.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "861FOZmhwMh9"
   },
   "source": [
    "Neural Network\n",
    "==============\n",
    "The neural network begins here. The first part of it is a pre-trained model A. Which was used in this task too. They will be used as a \"embedding layer\" for the writtings of each subject.\n",
    "\n",
    "The rest of the network will have the different subject writting embeddings as input of a RNN-type layer. Specifically to take into account th etemporal information. (It may probably change to CNN but we can have a look at that in the future work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_XZBtJZF_BKu"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, Input\n",
    "from keras.layers import Lambda, Flatten, RepeatVector, Permute, Multiply\n",
    "from keras.layers import LSTM, GRU, Bidirectional, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, Concatenate\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ipu3uOHJHEHc"
   },
   "outputs": [],
   "source": [
    "A_MODELS_PATH = SST_HOME + \"DL/models/\"\n",
    "MODEL_PATH = A_MODELS_PATH + \"copy_load_emb_False_num_classes_1_emb_size_300_trainable_emb_True_cnn_size_128_cnn_filter_3_pool_rnn_size_None_cell_type_LSTM_bidirectional_False_attention_False_dropout_0.5_dnn_size_32_batch_size_1024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 339952,
     "status": "ok",
     "timestamp": 1562652252831,
     "user": {
      "displayName": "PABLO RAEZ GARCIA-RETAMERO",
      "photoUrl": "",
      "userId": "03506494368259470344"
     },
     "user_tz": -120
    },
    "id": "mrA7B50MGnnL",
    "outputId": "8e462b11-1594-4622-b9cf-76cecfbe032e"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "a_model = load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yLlEE_S6p4s0"
   },
   "outputs": [],
   "source": [
    "a_model.layers.pop()\n",
    "a_model.layers.pop()\n",
    "# We pop two layers, being them the output and the last Dropout layers\n",
    "b_model = Sequential()\n",
    "\n",
    "for layer in a_model.layers:\n",
    "  b_model.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token_id_train = max([max(seq) for seq in train_x_df[\"TOKENIZED\"].tolist()])\n",
    "max_token_id_test = max([max(seq) for seq in test_x_df[\"TOKENIZED\"].tolist()])\n",
    "print(\"Max token ID in train dataset:\", max_token_id_train)\n",
    "print(\"Max token ID in test dataset:\", max_token_id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 4152\n",
    "oov_token_id = max_vocab_size - 1\n",
    "\n",
    "def replace_oov_tokens(sequences, oov_token_id):\n",
    "    return [[token if token < max_vocab_size else oov_token_id for token in seq] for seq in sequences]\n",
    "\n",
    "train_x_df[\"TOKENIZED\"] = replace_oov_tokens(train_x_df[\"TOKENIZED\"].tolist(), oov_token_id)\n",
    "test_x_df[\"TOKENIZED\"] = replace_oov_tokens(test_x_df[\"TOKENIZED\"].tolist(), oov_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GztJmHAtX0dI"
   },
   "outputs": [],
   "source": [
    "instances = np.array(train_x_df[\"TOKENIZED\"].tolist())\n",
    "test_instances = np.array(test_x_df[\"TOKENIZED\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FazNzbJ5P_qY"
   },
   "outputs": [],
   "source": [
    "embedded_text = b_model.predict(instances)\n",
    "embedded_text_test = b_model.predict(test_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NrJN1sd-dRt6"
   },
   "outputs": [],
   "source": [
    "train_x_df[\"EMBEDDINGS\"] = embedded_text.tolist()\n",
    "test_x_df[\"EMBEDDINGS\"] = embedded_text_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQPO8zzon_Fo"
   },
   "outputs": [],
   "source": [
    "# Set the size of the embeddings depending on the embeddings generated\n",
    "input_size = len(train_x_df.iloc[0][\"EMBEDDINGS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GB_bgNdes19A"
   },
   "outputs": [],
   "source": [
    "train_x_subjects = []\n",
    "train_y_subjects = []\n",
    "for subject in train_x_df.index.get_level_values(0).unique():\n",
    "  train_x_subjects.append(train_x_df.loc[subject][\"EMBEDDINGS\"].tolist())\n",
    "  train_y_subjects.append(train_y_df.loc[subject].values[0])\n",
    "  \n",
    "test_x_subjects = []\n",
    "test_y_subjects = []\n",
    "for subject in test_x_df.index.get_level_values(0).unique():\n",
    "  test_x_subjects.append(test_x_df.loc[subject][\"EMBEDDINGS\"].tolist())\n",
    "  test_y_subjects.append(test_y_df.loc[subject].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9yjWVQ9P_paf"
   },
   "source": [
    "Experiments Configuration\n",
    "====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g2Cn0hbsACO2"
   },
   "outputs": [],
   "source": [
    "tfidf = [False] # Not changing\n",
    "concept_emb = [False]  # Not Changing \n",
    "load_emb = [False]  # Not Changing\n",
    "num_classes = [None]  # Not changing\n",
    "emb_size = [None]  # Not changing, addressed later\n",
    "trainable_emb = [False]  # Not changing\n",
    "cnn_size = [[None]]  # Not changing\n",
    "cnn_filter = [[3]]  # Not changing\n",
    "rnn_size = [[64]]\n",
    "cell_type = [GRU, LSTM]\n",
    "bidirectional = [True, False]\n",
    "attention = [False]\n",
    "dropout = [0.5]\n",
    "dnn_size = [[32]]\n",
    "batch_size = [1]\n",
    "\n",
    "indexes = [\"load_emb\", \"emb_size\", \"trainable_emb\", \"cnn_size\", \"cnn_filter\", \"rnn_size\", \"cell_type\", \"bidirectional\", \"attention\", \"dropout\", \"dnn_size\", \"batch_size\"]\n",
    "param   = [load_emb, emb_size, trainable_emb, cnn_size, cnn_filter, rnn_size, cell_type, bidirectional, attention, dropout, dnn_size, batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uppogHJv_n4g"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def combine_params(param, indexes):\n",
    "  \"\"\"\n",
    "  This function is in charge of combining the parameters. This way we can perform a grid search automatically.\n",
    "  \n",
    "  Inputs: The array of different parameters, and the indexes for them.\n",
    "  \n",
    "  Outputs: The dictionary of different configurations made.\n",
    "  \"\"\"\n",
    "  combinations = list(itertools.product(*param))\n",
    "  param_combinations = [{k:v for k, v in zip(indexes, combination)}  for combination in combinations]\n",
    "  for p in param_combinations:\n",
    "    # The embeddings size must adapt to the embeddings loaded.\n",
    "    if p[\"load_emb\"]:\n",
    "      p[\"emb_size\"] = None\n",
    "    else:\n",
    "      p[\"embedding_matrix\"] = None\n",
    "  return param_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3Dmz50VBc71"
   },
   "outputs": [],
   "source": [
    "network_parameters = combine_params(param, indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y2SbKXnLBby8"
   },
   "outputs": [],
   "source": [
    "# We change the number of classes automatically\n",
    "for p in network_parameters:\n",
    "  p[\"max_length\"] = input_size\n",
    "  p[\"max_words\"] = max_words\n",
    "  try:\n",
    "    p[\"num_classes\"] = train_y.shape[1]\n",
    "  except IndexError:\n",
    "    p[\"num_classes\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NRJRN8W2-8bh"
   },
   "source": [
    "Neural Network\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9s-XZX6_zi1"
   },
   "outputs": [],
   "source": [
    "def add_embeddings(z, load, size, trainable, vocab_size, max_length, embedding_matrix): \n",
    "  \"\"\"\n",
    "  This method adds embeddings to the network.\n",
    "  \n",
    "  Returns the net with the embeddings added.\n",
    "  \"\"\"\n",
    "  print(size)\n",
    "  if size is None:\n",
    "    return z\n",
    "  \n",
    "  if load:\n",
    "    z = Embedding(vocab_size, size, input_length=max_length, weights=[embedding_matrix], trainable=trainable)(z)\n",
    "  else:\n",
    "    z = Embedding(vocab_size, size, input_length=max_length)(z)\n",
    "    \n",
    "  return z\n",
    "\n",
    "def add_cnn(z, size, filter_sizes, flatten):\n",
    "  \"\"\"\n",
    "  This method adds the CNN layers to the network.\n",
    "  \n",
    "  Returns the net with the CNN layers.\n",
    "  \"\"\"\n",
    "  conv_blocks = []\n",
    "  for filter_size in filter_sizes:\n",
    "    if filter_size is None:\n",
    "      return z\n",
    "    conv = None\n",
    "    for i, cnn_layer in enumerate(size):\n",
    "      if cnn_layer is None:\n",
    "        return z\n",
    "      conv = Conv1D(cnn_layer, filter_size, padding='valid', activation='relu', strides=1)(z if conv is None else conv)\n",
    "      # if (i + 1)  % 2 == 0:\n",
    "      conv = MaxPooling1D(pool_size=filter_size)(conv)        \n",
    "      \n",
    "    if flatten:\n",
    "      conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)         \n",
    " \n",
    "  z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "  \n",
    "  return z\n",
    "\n",
    "def add_rnn(z, size, bidirectional, cell_type, attention):\n",
    "  \"\"\"\n",
    "  This method adds the RNN layers to the network. It also adds an attention layer if intended.\n",
    "  \n",
    "  Returns the net with the RNN & Attention added.\n",
    "  \"\"\"\n",
    "  for i, rsz in enumerate(size):\n",
    "    if rsz is None:\n",
    "      return z\n",
    "    if not bidirectional:\n",
    "      if i < len(size) - 1:\n",
    "        z = cell_type(rsz, return_sequences=True)(z)\n",
    "      else:\n",
    "        z = cell_type(rsz, return_sequences=attention)(z)\n",
    "    else:\n",
    "      if i < len(size) - 1:\n",
    "        z = Bidirectional(cell_type(rsz, return_sequences=True))(z)\n",
    "      else:\n",
    "        z = Bidirectional(cell_type(rsz, return_sequences=attention))(z)\n",
    "\n",
    "  if attention:\n",
    "    z = add_attention(z)\n",
    "\n",
    "  return z\n",
    "\n",
    "def add_dnn(z, size, dropout, activation=\"relu\"):\n",
    "  \"\"\"\n",
    "  This method adds the DNN layers to the network.\n",
    "  \n",
    "  Returns the net with the DNN layers added.\n",
    "  \"\"\"\n",
    "  for fsz in size:\n",
    "    if fsz is None:\n",
    "      return z\n",
    "\n",
    "    z = Dense(fsz, activation=activation)(z)\n",
    "    z = Dropout(dropout)(z)\n",
    "    \n",
    "  return z\n",
    "\n",
    "def add_attention(activations):\n",
    "  \"\"\"\n",
    "  This method adds an attention layer.\n",
    "  \n",
    "  Returns the model with the attention layer.\n",
    "  \"\"\"\n",
    "  \n",
    "  size =  K.int_shape(activations)[-1]\n",
    "  attention = BatchNormalization()(activations)\n",
    "  attention = Dense(1, activation='tanh')(attention)\n",
    "  print(attention)\n",
    "  attention = Flatten()(attention)\n",
    "  attention = Activation('softmax')(attention)\n",
    "  attention = RepeatVector(size)(attention)\n",
    "  attention = Permute([2, 1])(attention)\n",
    "  \n",
    "  z = Multiply()([activations, attention])\n",
    "  z = Lambda(lambda xin: K.sum(xin, axis=-2), output_shape=(size,))(z)\n",
    "\n",
    "  return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "am-ZSm1d_2tO"
   },
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "  \"\"\"\n",
    "  This method creates a network model with the parameters given.\n",
    "  \n",
    "  Returns the uncompiled model.\n",
    "  \"\"\"\n",
    "  # We changed a bit the model to get variable length inputs.\n",
    "  inputs = Input(name='inputs',shape=(None, params[\"max_length\"]))\n",
    "  \n",
    "  z = add_embeddings(inputs, params[\"load_emb\"], params[\"emb_size\"], params[\"trainable_emb\"], params[\"max_words\"] ,params[\"max_length\"], params[\"embedding_matrix\"])\n",
    "  \n",
    "  z = add_cnn(z, params[\"cnn_size\"], params[\"cnn_filter\"], [None] == params[\"rnn_size\"])\n",
    "  \n",
    "  z = add_rnn(z, params[\"rnn_size\"], params[\"bidirectional\"], params[\"cell_type\"], params[\"attention\"])\n",
    "  \n",
    "  z = add_dnn(z, params[\"dnn_size\"], params[\"dropout\"])\n",
    "  \n",
    "  outputs = Dense(params[\"num_classes\"], activation='sigmoid', name='output_layer')(z)\n",
    "  \n",
    "  net_model = Model(inputs=inputs,outputs=outputs)\n",
    "  \n",
    "  return net_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58YY8qkn_6q2"
   },
   "outputs": [],
   "source": [
    "def create_models(network_parameters, verbose=False):\n",
    "  model_list = []\n",
    "  for net_p in network_parameters:\n",
    "    print(net_p)\n",
    "    m = create_model(net_p)\n",
    "    m.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
    "    model_list.append(m)\n",
    "    \n",
    "    if verbose:\n",
    "      m.summary()\n",
    "      \n",
    "  return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RELUfIoi_8PL"
   },
   "outputs": [],
   "source": [
    "model_list = create_models(network_parameters, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1k_1Ff-iwRbV"
   },
   "source": [
    "# TB Colab Callback\n",
    "We rewrite the tensorboardcolab callbacks to create different sessions depending on the variables our trainings have. This helps to differentiate the models in tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3jsRRzrFv1NH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "\n",
    "class TensorBoardColabCallback(TensorBoard):\n",
    "    def __init__(self, tbc=None, write_graph=True, name=None, **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "\n",
    "        if tbc is None:\n",
    "            return\n",
    "\n",
    "        log_dir = tbc.get_graph_path()\n",
    "\n",
    "        training_log_dir = os.path.join(log_dir, 'training_{}'.format(name))\n",
    "        super(TensorBoardColabCallback, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation_{}'.format(name))\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TensorBoardColabCallback, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "\n",
    "        for name, value in val_logs.items():\n",
    "            # print('val_logs:',epoch, name, value)\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TensorBoardColabCallback, self).on_epoch_end(epoch, logs)\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TensorBoardColabCallback, self).on_train_end(logs)\n",
    "        self.val_writer.close()\n",
    "\n",
    "tb.TensorBoardColabCallback = TensorBoardColabCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hd7YRWvKvXAj"
   },
   "outputs": [],
   "source": [
    "def define_callbacks(name):\n",
    "  # Define the callbacks\n",
    "  #tbc_callback = tb.TensorBoardColabCallback(tbc, name=name)  # , histogram_freq=1)\n",
    "   \n",
    "  callbacks = [\n",
    "      ReduceLROnPlateau(),\n",
    "      EarlyStopping(patience=4),\n",
    "      #tbc_callback\n",
    "  ]\n",
    "  return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C5g5kvMmJgl3"
   },
   "outputs": [],
   "source": [
    "# Create an array of names\n",
    "network_names = []\n",
    "for p in network_parameters:\n",
    "  name = \"load_emb_{}_num_classes_{}_emb_size_{}_trainable_emb_{}_cnn_size_{}_cnn_filter_{}_pool_rnn_size_{}_cell_type_{}_bidirectional_{}_attention_{}_dropout_{}_dnn_size_{}_batch_size_{}\".format(\n",
    "      p[\"load_emb\"], p[\"num_classes\"], p[\"emb_size\"], p[\"trainable_emb\"], p[\"cnn_size\"], p[\"cnn_filter\"], p[\"rnn_size\"], \n",
    "      str(p[\"cell_type\"]).split(\".\")[-1].replace(\"'\", \"\").replace(\">\", \"\"), p[\"bidirectional\"],\n",
    "      p[\"attention\"], p[\"dropout\"], p[\"dnn_size\"], p[\"batch_size\"])\n",
    "  name = name.replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"-\")\n",
    "  network_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQypOjW-Jz6v"
   },
   "outputs": [],
   "source": [
    "for i, net_model in enumerate(model_list):\n",
    "  # Set a name for the model based on the tweaked parameters\n",
    "  p = network_parameters[i]\n",
    "  name = network_names[i]\n",
    "  model_path = SST_HOME+\"DL/models/C/\" + name\n",
    "\n",
    "  # If the model exists, don't compute it again.\n",
    "  if os.path.isfile(model_path):\n",
    "    continue\n",
    "    \n",
    "  print(\"\\n\\n********************************************\\n\")    \n",
    "  print(name)\n",
    "  callbacks = define_callbacks(name)\n",
    "  # Fit the model and extract its data\n",
    "  for epoch in range(20):\n",
    "    print(\"Epoch {}\\n\".format(epoch))\n",
    "    for subject, label in zip(train_x_subjects, train_y_subjects):\n",
    "      history = net_model.fit(np.array([subject]), np.array([label]), callbacks=callbacks, class_weight={0: 0.11, 1: 0.89})\n",
    "      # Print the loss and accuracy of the training and validation sets for each epoch\n",
    "      print(history.history['loss'])\n",
    "      #print(history.history['val_loss'])\n",
    "      print(history.history['binary_accuracy'])\n",
    "      #print(history.history['val_binary_accuracy'])\n",
    "    \n",
    "  # And save the model\n",
    "  net_model.save(model_path)\n",
    "  \n",
    "# To free memory from the gpu\n",
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Nh8H3WyQmjg"
   },
   "source": [
    "Evaluation\n",
    "========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Lxy8qKkwD-1"
   },
   "outputs": [],
   "source": [
    "for trheshold in np.arange(0.1, 1.5, 0.1):\n",
    "  print(\"THRESHOLD: {}\\n**********************\".format(trheshold))\n",
    "  from sklearn.metrics import classification_report\n",
    "  predictions_list = []\n",
    "  for i, net_model in enumerate(model_list):\n",
    "    print(\"\\n\\n********************************************\\n\")\n",
    "    print(network_names[i])\n",
    "    model_path = SST_HOME+\"DL/models/C/\" + network_names[i]\n",
    "    try:\n",
    "      net_model = load_model(model_path)\n",
    "    except ValueError:\n",
    "      print(\"The model {} was not loaded correctly\".format(name))\n",
    "      continue\n",
    "    except OSError:\n",
    "      print(\"The model {} does not exist\".format(name))\n",
    "      continue\n",
    "\n",
    "    predictions = []\n",
    "    for subject in test_x_subjects:\n",
    "      predictions.append(net_model.predict(np.array([subject])))\n",
    "\n",
    "    predictions = np.array([0 if prediction < trheshold else 1 for prediction in predictions])\n",
    "    predictions_list.append(predictions)\n",
    "    # measuring performance on test set\n",
    "    cr=classification_report(test_y_subjects, predictions)\n",
    "    print(cr)\n",
    "    # Release memory\n",
    "    K.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script C-Copy2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
